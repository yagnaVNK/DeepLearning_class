{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F07HKWTk8VBx"
      },
      "source": [
        "#PyTorch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4uafWiZ8cVZ",
        "outputId": "059df7e2-413a-4330-8e48-4380374b1925"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1597b9ff890>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(0) # for reproducability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S-vhnqa8v1p"
      },
      "source": [
        "##Tensor Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JLNnbQTI8Rav"
      },
      "outputs": [],
      "source": [
        "arr = [1,2]\n",
        "tensor = torch.tensor(arr)\n",
        "val = 2.0\n",
        "tensor = torch.tensor(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UjQX09lMWloJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np_arr = np.array([1,2])\n",
        "x_t = torch.from_numpy(np_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nAuimXkkW25x"
      },
      "outputs": [],
      "source": [
        "zeros_t = torch.zeros((2,3)) # Returns 2x3 tensor of zeros\n",
        "ones_t = torch.ones((2,3)) # Returns 2x3 tensor of ones\n",
        "rand_t = torch.randn((2,3)) # Returns 2x3 tensor of random numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyaxqW2r843z"
      },
      "source": [
        "## Tensor Attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVdTfSyb8rzG",
        "outputId": "f4188b9e-394e-4515-b71e-de3ba87e10ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zeros_t.shape # Returns torch.Size([2, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3JD0VX6838x",
        "outputId": "9a01db1f-237d-4c99-8c73-686c3126b570"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_t = torch.tensor(2.0)\n",
        "x_t.dtype # Returns torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LsAfVEf2BssB"
      },
      "outputs": [],
      "source": [
        "arr = [1,2]\n",
        "x_t = torch.tensor(arr, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re9-RKyVBvm1",
        "outputId": "d3bdbe09-821e-4de6-a971-044e37181b83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_t.device # Returns device(type='cpu') by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q4nLgr0hBypK"
      },
      "outputs": [],
      "source": [
        "# PyTorch will use GPU if it's available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "arr = [1,2]\n",
        "x_t = torch.tensor(arr, dtype=torch.float32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HMvlzUQFB1m5"
      },
      "outputs": [],
      "source": [
        "x_t = x_t.to(device, dtype=torch.int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SadKNln871T"
      },
      "source": [
        "## Tensor Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gVmKbPR089YB"
      },
      "outputs": [],
      "source": [
        "c = 10\n",
        "x_t = x_t*c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NJkZzRTBKzi",
        "outputId": "b8dd5219-0cdb-4690-96c8-412accb956c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1_t = torch.zeros((1,2))\n",
        "x2_t = torch.ones((1,2))\n",
        "x1_t + x2_t\n",
        "# returns tensor([[1., 1.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQdOD6J7BNjW",
        "outputId": "d5e6d950-f1a2-4741-87aa-eb54c2a4dbac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 9, 12, 15],\n",
              "        [19, 26, 33]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1_t = torch.tensor([[1,2],[3,4]])\n",
        "x2_t = torch.tensor([[1,2,3],[4,5,6]])\n",
        "torch.matmul(x1_t, x2_t) # Returns tensor([[9,12,15],[19,26,33]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C5gv5sWBR6p",
        "outputId": "c7c1df69-10dd-4ab1-952e-0c95fd4121ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[3, 7, 9],\n",
            "         [2, 4, 5]],\n",
            "\n",
            "        [[8, 6, 2],\n",
            "         [3, 9, 1]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(4)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i,j,k = 0,1,1\n",
        "x3_t = torch.tensor([[[3,7,9],[2,4,5]],[[8,6,2],[3,9,1]]])\n",
        "print(x3_t)\n",
        "# out:\n",
        "# tensor([[[3, 7, 9],\n",
        "#          [2, 4, 5]],\n",
        "#         [[8, 6, 2],\n",
        "#          [3, 9, 1]]])\n",
        "\n",
        "x3_t[i,j,k]\n",
        "# out:\n",
        "# tensor(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XMe_Ls7BW1A",
        "outputId": "b694d7cb-b992-4483-8cb3-466e68f80227"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[3, 7, 9],\n",
              "        [2, 4, 5]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x3_t[0] # Returns the matrix at position 0 in tensor\n",
        "x3_t[0,:,:] # Also returns the matrix at position 0 in tensor!\n",
        "# out:\n",
        "# tensor([[3, 7, 9],\n",
        "#         [2, 4, 1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1N7VBdhBZqt",
        "outputId": "53e4693d-421b-4327-a7cd-3e3903770527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2, 4, 5]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x3_t[0,1:3,:]\n",
        "# returns tensor([[2, 4, 5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ne5lWR2JBb-a"
      },
      "outputs": [],
      "source": [
        "x3_t[0,1,2] = 1\n",
        "# out:\n",
        "# tensor([[[3, 7, 9],\n",
        "#          [2, 4, 1]],\n",
        "\n",
        "#         [[8, 6, 2],\n",
        "#          [3, 9, 1]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bKpC3twXBeBe"
      },
      "outputs": [],
      "source": [
        "x_t = torch.randn(2,3,4)\n",
        "sub_tensor = torch.randn(2,4)\n",
        "x_t[0,1:3,:] = sub_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7WyDzjhvBiol"
      },
      "outputs": [],
      "source": [
        "x_t[0,1:3,:] = 1\n",
        "sub_tensor = torch.randn(1,4)\n",
        "x_t[0,1:3,:] = sub_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD8EA1gw9ApD"
      },
      "source": [
        "#Gradients in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz6ksmHL9DRD",
        "outputId": "18740ec9-73a1-49cf-8920-2c43a108d204"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(4.), tensor(6.), tensor(3.))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "z = torch.tensor(1.5, requires_grad=True)\n",
        "f = x**2+y**2+z**2\n",
        "f.backward()\n",
        "x.grad, y.grad, z.grad\n",
        "# out:\n",
        "# (tensor(4.), tensor(6.), tensor(3.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-2brO89ES0"
      },
      "source": [
        "# The PyTorch nn module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yDw2Kvg49GgO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gkoyexGYAC7Q"
      },
      "outputs": [],
      "source": [
        "in_dim, out_dim = 256, 10\n",
        "vec = torch.randn(256)\n",
        "layer = nn.Linear(in_dim, out_dim, bias=True)\n",
        "out = layer(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "172mP5cuAFyd"
      },
      "outputs": [],
      "source": [
        "W = torch.rand(10,256)\n",
        "b = torch.zeros(10,1)\n",
        "out = torch.matmul(W, vec) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mtNoAAE6AKcv"
      },
      "outputs": [],
      "source": [
        "in_dim, feature_dim, out_dim = 784, 256, 10\n",
        "vec = torch.randn(784)\n",
        "layer1 = nn.Linear(in_dim, feature_dim, bias=True)\n",
        "layer2 = nn.Linear(feature_dim, out_dim, bias=True)\n",
        "out = layer2(layer1(vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MLRoVWgzANnr"
      },
      "outputs": [],
      "source": [
        "relu = nn.ReLU()\n",
        "out = layer2(relu(layer1(vec)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6bBeoIeMAR39"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier(nn.Module):\n",
        "  def __init__(self, in_dim, feature_dim, out_dim):\n",
        "    super(BaseClassifier, self).__init__()\n",
        "    self.layer1 = nn.Linear(in_dim, feature_dim, bias=True)\n",
        "    self.layer2 = nn.Linear(feature_dim, out_dim, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    out = self.layer2(x)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "snixCbP3Abuk"
      },
      "outputs": [],
      "source": [
        "no_examples = 10\n",
        "in_dim, feature_dim, out_dim = 784, 256, 10\n",
        "x = torch.randn((no_examples, in_dim))\n",
        "classifier = BaseClassifier(in_dim, feature_dim, out_dim)\n",
        "out = classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vg3QNC3TAe9D"
      },
      "outputs": [],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "target = torch.tensor([0,3,2,8,2,9,3,7,1,6])\n",
        "computed_loss = loss(out, target)\n",
        "computed_loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1p_J0g4ut3F",
        "outputId": "d7c3f383-6685-4085-d72c-7f8585525e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 784])\n",
            "torch.Size([256])\n",
            "torch.Size([10, 256])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for p in classifier.parameters():\n",
        "  print(p.shape)\n",
        "\n",
        "# out:\n",
        "# torch.Size([256, 784])\n",
        "# torch.Size([256])\n",
        "# torch.Size([10, 256])\n",
        "# torch.Size([10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DGb30o-VAlyz"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "lr = 1e-3\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WsVMNnSnAoey"
      },
      "outputs": [],
      "source": [
        "optimizer.step() # Updates parameters via SGD\n",
        "optimizer.zero_grad() # Zeroes out gradients between minibatches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCgfC7oM9H4I"
      },
      "source": [
        "#PyTorch Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "e4eOrEzr9Nni"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gRgBTOpX0sRp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "labels = np.array([2, 0, 4, 1])\n",
        "np.save('labels',labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvu8i9QX06LN",
        "outputId": "aa6cfb48-a801-4b2f-cc70-b89651d5d564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 0, 4, 1])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_1 = np.load('labels.npy')\n",
        "labels_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGRIi9Cpgqf0"
      },
      "source": [
        "### First download files for use in custom ImageDataset example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "myX-sYoDeL-0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file data already exists.\n",
            "A subdirectory or file data\\train already exists.\n"
          ]
        }
      ],
      "source": [
        "!mkdir  data\n",
        "!mkdir  data\\train\n",
        "\n",
        "!wget -O data/train/img_0.jpg -nc -q https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/raw/master/ch05_implementing_nn_pytorch/data/train/img_0.jpg\n",
        "!wget -O data/train/img_1.jpg -nc -q https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/raw/master/ch05_implementing_nn_pytorch/data/train/img_1.jpg\n",
        "!wget -O data/train/img_2.jpg -nc -q https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/raw/master/ch05_implementing_nn_pytorch/data/train/img_2.jpg\n",
        "!wget -O data/train/img_3.jpg -nc -q https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/raw/master/ch05_implementing_nn_pytorch/data/train/img_3.jpg\n",
        "!wget -O data/train/labels.npy -nc -q https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/raw/master/ch05_implementing_nn_pytorch/data/train/labels.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "3Cy4e5v4-nJY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, img_dir, label_file):\n",
        "    super(ImageDataset, self).__init__()\n",
        "    self.img_dir = img_dir\n",
        "    self.labels = torch.tensor(np.load(label_file, allow_pickle=True),device=device)\n",
        "    self.transforms = transforms.ToTensor()\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img_pth = os.path.join(self.img_dir, \"img_{}.jpg\".format(idx))\n",
        "    img = Image.open(img_pth)\n",
        "    img = self.transforms(img).flatten()\n",
        "    label = self.labels[idx]\n",
        "    return {\"data\":img, \"label\":label}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "9P5g3VFg-uwc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.ImageDataset object at 0x00000159C2B9D910>\n"
          ]
        }
      ],
      "source": [
        "train_dataset = ImageDataset(img_dir='./data/train/',\n",
        "                             label_file='./data/train/labels.npy')\n",
        "print(train_dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=4, \n",
        "                          shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "245Aiav8-zdU",
        "outputId": "78e046bf-5839-4e07-c564-94f32d715d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([1, 0, 4, 2], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "for minibatch in train_loader:\n",
        "  data, labels = minibatch['data'], minibatch['label']\n",
        "  print(data)\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhIl1Li_9Pe7"
      },
      "source": [
        "#Building the MNIST Classifer in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEaA22tUmZnJ",
        "outputId": "21b628a4-f380-4706-d048-f88b7e1c63d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1597b9ff890>"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# For reproducability\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "uaGRqlxP9VCs"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier(nn.Module):\n",
        "  def __init__(self, in_dim, feature_dim, out_dim):\n",
        "    super(BaseClassifier, self).__init__()\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(in_dim, feature_dim, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(feature_dim, out_dim, bias=True)\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.classifier(x)\n",
        "    \n",
        "\n",
        "# Load in MNIST dataset from PyTorch\n",
        "train_dataset = MNIST(\".\", train=True, \n",
        "                      download=True, transform=ToTensor())\n",
        "test_dataset = MNIST(\".\", train=False, \n",
        "                     download=True, transform=ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, \n",
        "                         batch_size=64, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qpb4Tb44oho"
      },
      "source": [
        "# NOTE: \n",
        "The train() function below will take approx. 5-6 minutes to run with epochs = 40."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "m0P-MpP-9lfl"
      },
      "outputs": [],
      "source": [
        "# Instantiate model, optimizer, and hyperparameter(s)\n",
        "device = 'cpu'\n",
        "in_dim, feature_dim, out_dim = 784, 256, 10\n",
        "lr=1e-2\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs=40\n",
        "classifier = BaseClassifier(in_dim, feature_dim, out_dim)\n",
        "classifier.to(device)\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=lr)\n",
        "\n",
        "def train(classifier=classifier,\n",
        "          optimizer=optimizer,\n",
        "          epochs=epochs,\n",
        "          loss_fn=loss_fn):\n",
        "\n",
        "  classifier.train()\n",
        "  loss_lt = []\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for minibatch in train_loader:\n",
        "      data, target = minibatch\n",
        "      data, target = data.to(device) ,  target.to(device)\n",
        "      data = data.flatten(start_dim=1)\n",
        "      out = classifier(data)\n",
        "      computed_loss = loss_fn(out, target)\n",
        "      computed_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      # Keep track of sum of loss of each minibatch\n",
        "      running_loss += computed_loss.item()\n",
        "    loss_lt.append(running_loss/len(train_loader))\n",
        "    print(\"Epoch: {} train loss: {}\".format(epoch+1, running_loss/len(train_loader)))\n",
        "\n",
        "  plt.plot([i for i in range(1,epochs+1)], loss_lt)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Training Loss\")\n",
        "  plt.title(\n",
        "      \"MNIST Training Loss: optimizer {}, lr {}\".format(\"SGD\", lr))\n",
        "  plt.show()\n",
        "\n",
        "  # Save state to file as checkpoint\n",
        "  torch.save(classifier.state_dict(), 'mnist.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "pMiCCu8Nln8w"
      },
      "outputs": [],
      "source": [
        "def test(classifier=classifier, \n",
        "          loss_fn = loss_fn):\n",
        "  classifier.eval()\n",
        "  accuracy = 0.0\n",
        "  computed_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data = data.flatten(start_dim=1)\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          out = classifier(data)\n",
        "          _, preds = out.max(dim=1)\n",
        "\n",
        "          # Get loss and accuracy\n",
        "          computed_loss += loss_fn(out, target)\n",
        "          accuracy += torch.sum(preds==target)\n",
        "          \n",
        "      print(\"Test loss: {}, test accuracy: {}\".format(\n",
        "          computed_loss.item()/(len(test_loader)*64), accuracy*100.0/(len(test_loader)*64)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "g79BDGlp85Uc",
        "outputId": "97059680-2635-47c1-e9ee-a56aaf86c5f6"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\kaasa\\Documents\\DeepLearning\\DeepLearning_class\\Nueral Network\\Ch05_PyTorch.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train()\n",
            "\u001b[1;32mc:\\Users\\kaasa\\Documents\\DeepLearning\\DeepLearning_class\\Nueral Network\\Ch05_PyTorch.ipynb Cell 53\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m data, target \u001b[39m=\u001b[39m minibatch\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m out \u001b[39m=\u001b[39m classifier(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m computed_loss \u001b[39m=\u001b[39m loss_fn(out, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m computed_loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\kaasa\\Documents\\DeepLearning\\DeepLearning_class\\Nueral Network\\Ch05_PyTorch.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/DeepLearning/DeepLearning_class/Nueral%20Network/Ch05_PyTorch.ipynb#Y103sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n",
            "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3UsxvLd874B",
        "outputId": "50fd10a0-75d5-4321-89da-89216ebf4d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.0018742650177828067, test accuracy: 96.0688705444336\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ch05_PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
